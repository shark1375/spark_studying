{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"name":"4.Structural API Intro.ipynb","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"BT1BcPebJ8X0"},"source":["- 구조적 API는 비정형 로그 파일부터 반정형 csv, 정형적인 파퀫까지 다양한 데이를 처리할 수 있음\n","- 다음과 같은 세 가지 분산 컬렉션 API가 존재함\n","    - 1) Dataset\n","    - 2) DataFrame\n","    - 3) SQL table & View\n","- 배치 및 스트리밍 처리에서 구조적 API를 사용할 수 있으며, 구조적 API를 통해 배치 작업을 스트리밍 작업으로 변환할 수 있음(반대도 OK)\n","- 구조적 API는 데이터 흐름을 정의하는 기본 추상화 개념\n","- 이번 장에서 반드시 이해해야하는 세 가지 개념\n","    - 1) Typed/Untyped API의 개념 및 차이\n","    - 2) 핵심 용어\n","    - 3) 스파크가 구조적 API의 데이터 흐름을 해석하고 클러스터에서 실행하는 방식"]},{"cell_type":"markdown","metadata":{"id":"iC8aUrphJ8X0"},"source":["- Spark : 트랜스포메이션의 처리 과정을 정의하는 프로그래밍 모델\n","- 사용자가 정의한 다수의 T는 지향성 비순환 그래프(DGA)로 표현되는 명령을 만들어냄\n","- 액션은 하나의 job을 클러스터에서 실행하기 위해 스테이지와 태스크로 나누고 DAG처리 프로세스를 실행함\n","- T와 A로 다루는 논리적 구조가 DataFrame & DataSet임\n","- 새로운 DF나 DES를 만드려면 트랜스포메이션을 호출해야함\n","    - 그리고 연산을 시작하거나 사용 언어에 맞는 데이터 타입으로 변환하려면 액션을 호출해야함\n","   "]},{"cell_type":"markdown","metadata":{"id":"cOnrDX4TJ8X0"},"source":["# 4.1.DataFrame & Dataset\n","- 둘 다 잘 정의된 로우와 컬럼을 가지는 분산 테이블 형태의 컬렉션\n","    - 각 컬럼은 다른 컬럼과 동일한 수의 로우를 가져야함\n","    - 또한 각 컬렉션의 모든 로우는 같은 데이터타입 정보를 가지고 있어야함\n","- 둘 다 결과를 생성하기 위한 지연연산의 실행 계획이며, 불변성을 가짐\n","- DF에 A를 호출하면 스파크는 T를 실행하고, 결과를 반환함\n","    - 즉 이 과정은 사용자가 원하는 결과를 얻기 위해 로우와 컬럼을 처리하는 방법에 대한 계획을 나타냄\n","    - *기본적으로 Table, View는 DF와 동일함*\n","    - *대신 Table, View는 DF코드 대신 SQL을 활용함*"]},{"cell_type":"markdown","metadata":{"id":"xXX7-ZTFJ8X0"},"source":["# 4.2.Schema\n","- DF의 컬럼명과 데이터타입을 정의함\n","- 데이터 소스에서 얻거나(Schema on Read), 직접 정의할 수 있음\n","- 스키마는 여러 데이터 타입으로 구성되므로, 어떤 데이터 타입이 어느 위치에 있는지 정의할 수 있어야함"]},{"cell_type":"markdown","metadata":{"id":"HRfCbTKAJ8X0"},"source":["# 4.3.Introduction To Structural Datatype Of Spark\n","- 실행 계획의 수립과 처리에 사용하는 자체 데이터타입정보를 가지고 있는 카탈리스트 엔진을 이용함\n","    - 이는 다양한 실행 최적화 기능을 제공함\n","- 또한 위 엔진을 통해 자체 데이터타입을 제공하는 여러 언어 API와 매핑할 수 있음\n","    - 따라서 파이썬이나 R로 스파크의 구조적 API를 이용하더라도, 대다수 연산은 Spark Datatype을 이용하게 됨"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ixGnfQYkJ8X0","executionInfo":{"status":"ok","timestamp":1606538556937,"user_tz":-540,"elapsed":46024,"user":{"displayName":"Seohyeon Youn","photoUrl":"","userId":"11183665362829847919"}},"outputId":"eada790d-714a-4ddb-dc31-1c3c722e2ffd"},"source":["from pyspark.sql import SparkSession\n","spark = SparkSession.builder.appName('abc').getOrCreate()"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Collecting pyspark\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f0/26/198fc8c0b98580f617cb03cb298c6056587b8f0447e20fa40c5b634ced77/pyspark-3.0.1.tar.gz (204.2MB)\n","\u001b[K     |████████████████████████████████| 204.2MB 65kB/s \n","\u001b[?25hCollecting py4j==0.10.9\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9e/b6/6a4fb90cd235dc8e265a6a2067f2a2c99f0d91787f06aca4bcf7c23f3f80/py4j-0.10.9-py2.py3-none-any.whl (198kB)\n","\u001b[K     |████████████████████████████████| 204kB 35.9MB/s \n","\u001b[?25hBuilding wheels for collected packages: pyspark\n","  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyspark: filename=pyspark-3.0.1-py2.py3-none-any.whl size=204612243 sha256=45f43ee170198c84ff90d408836c717d483246dfd4c52aeab0828f7bb550f618\n","  Stored in directory: /root/.cache/pip/wheels/5e/bd/07/031766ca628adec8435bb40f0bd83bb676ce65ff4007f8e73f\n","Successfully built pyspark\n","Installing collected packages: py4j, pyspark\n","Successfully installed py4j-0.10.9 pyspark-3.0.1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"PyNyfGmiJ8X1"},"source":["df = spark.range(500).toDF(\"number\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xFkarB4mJ8X1","outputId":"62393f39-fb17-4bca-a16e-65015552af14"},"source":["df.select(df[\"number\"] + 10)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["DataFrame[(number + 10): bigint]"]},"metadata":{"tags":[]},"execution_count":7}]},{"cell_type":"markdown","metadata":{"id":"gPBS1lA1J8X3"},"source":["- 이렇게 pyspark로 실행하더라도 결국 scala기반으로 진행됨\n","- 이렇게 덧셈연산이 수행될 수 있는 이유는 스파크가 지원하는 언어를 이용해 작성된 표현식을 카탈리스트 엔진에서 스파크의 데이터타입으로 변환하여 명령을 처리하기 때문"]},{"cell_type":"markdown","metadata":{"id":"0K4TDZSgJ8X3"},"source":["### 4.3.1.DataFrame v.s. Dataset\n","- 본질적으로 구조적 API에는 \"비타입형\" DataFrame과 \"타입형\"인 Dataset이 있음\n","    - DF는 스키마에 명시된 데이터타입과 실제 데이터 타입의 일치여부를 런타임이 되어서야 확인할 수 있음\n","    - 하지만 DataSet은 컴파일 단계에서 바로 확인함\n","        - 다만, DataSet은 scala와 Java에서만 지원함\n","- 이 책의 예제 대부분은 DF를 사용함\n","- Spark DF는 Row 타입으로 구성된 Dataset임\n","    - Row타입은 스파크가 사용하는 **연산최적화 인메모리포맷**의 내부적인 표현방식임\n","    - 이를 통해 가비지 컬렉션과 ㅈ객체 초기화 부하가 있는 JVM데이터 타입을 사용하는 대신, 자체 데이터 포맷을 이용 -> 따라서 **효율성 UP!!**\n","- 파이썬과 R에서는 Dataset을 이용할 수는 없지만, 최적화된 포맷인 DF로 처리할 수 있음\n"]},{"cell_type":"markdown","metadata":{"id":"wXIousDkJ8X3"},"source":["### 4.3.2.Column\n","- 스파크의 컬럼은 테이블의 컬럼이라고 생각하면 됨"]},{"cell_type":"markdown","metadata":{"id":"pNAs5y64J8X3"},"source":["### 4.3.3.Row\n","- 데이터 레코드임\n","- SQL, RDD, DataSource에서 얻거나 만들 수 있음"]},{"cell_type":"code","metadata":{"id":"RXq0OnYUJ8X3","outputId":"13cdb28d-b60e-4cde-8990-f7c4b05d23ba"},"source":["spark.range(2).collect()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[Row(id=0), Row(id=1)]"]},"metadata":{"tags":[]},"execution_count":3}]},{"cell_type":"markdown","metadata":{"id":"Le_sSCK9J8X3"},"source":["### 4.3.4.DataType Of Spark\n","- 여러가지 내부 데이터 타입을 지원함\n","- 특정 데이터타입의 컬럼을 초기화하고 정의하는 방법\n","    - 스파크와 파이썬의 데이터타입 매칭 여부는 p.116d에서 확인할 수 있음"]},{"cell_type":"code","metadata":{"id":"QFIyiRB7J8X3"},"source":["from pyspark.sql.types import *\n","b = ByteType()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Bse94wtJJ8X3"},"source":["![image.png](attachment:image.png)\n","![image-2.png](attachment:image-2.png)"]},{"cell_type":"markdown","metadata":{"id":"iURM94saJ8X4"},"source":["# 4.4. Procedure Of Structural API\n","- 스파크 코드가 실제로 어떻게 처리되는지 확인\n","- 사용자 코드에서 실행 코드로 변환되는 과정은 아래와 같음\n","    - 1) DF, DS, SQL을 통해 코드 작성\n","        - console이나 spark-submit셀 스크립트로 실행\n","    - 2) 정상적인 코드라면 스파크가 **논리적 실행계획**으로 변환함\n","        - 카탈리스트 옵티마이저가 해당 코드를 넘겨받음\n","    - 3) 스파크는 **논리적 계획**을 **물리적 실행계획**으로 변환하며 그 과정에서 최적화 진행\n","        - 카탈리스트 옵티마이저가 실행계획을 생성\n","    - 4) 스파크는 클러스터에서 **물리적 실행계획**을 실행함(RDDcjfl)\n","        - 스파크가 코드를 실행하고 결과를 반환\n","- ![image.png](attachment:image.png)"]},{"cell_type":"markdown","metadata":{"id":"x8TRBYvSJ8X4"},"source":["### 4.4.1. Logical Plan\n","- 첫 실행 단계로, 사용자 코드를 논리적 실행계획으로 변환함\n","- ![image.png](attachment:image.png)\n","    - 위 그림에서는 Optimized Logical Plan까지가 Logical Plan 단계\n","- 논리적 실행계획단계에서는 추상적인 T만 표현함\n","    - 즉, 드라이버나 익스큐터의 정보를 고려하지 않음\n","- 사용자의 다양한 표현식을 최적화된 버전으로 변환함\n","    - 이 과정에서 **검증 전 논리적 실행 계획(Unresolved Logical Plan)**으로 변환됨\n","    - 오로지 코드의 유효성과 테이블/컬럼의 존재 여부만 판단하는 과정으로 *실행계획을 검증하지는 않은 상태*\n","- 그리고 **스파크 분석기**가 컬럼과 테이블을 검증하기 위해 **카탈로그**, 모든 테이블의 저장소, DFwjdqhfmf ghkfdydgkrp ehla\n","    - 만약 해당 카탈로그에 필요 테이블이나 컬럼이 존재하지 않는 다면, 검증 전 논리적 실행계획 자체가 만들어지지 않음\n","- 검증 결과는 카탈리스트 옵티마이저로 전달\n","    - **카탈리스트 옵티마이저** : *조건절 푸시다운*이나 *선택절 구문*을 이용해 논리적 실행 계획을 최적화하는 규칙의 모음\n"]},{"cell_type":"markdown","metadata":{"id":"JZExSfiSJ8X4"},"source":["### 4.4.2.Physical Plan\n","- 스파크 실행계획이라고도 불림\n","- 논리적 실행계획을 클러스터 환경에서 실행하는 방법을 정의함\n","- 다양한 물리적 실행 전략을 생성하고 코스트 모델을 이용하여 비교한 후 최적의 모델 선택\n","- ![image.png](attachment:image.png)\n","- 물리적 실행계획은 일련의 RDD와 T로 변환됨\n","    - 그 후 DF, DS, SQL로 정의된 쿼리를 RDD T로 컴파일함"]},{"cell_type":"markdown","metadata":{"id":"EWN74w90J8X4"},"source":["### 4.4.3.Execute\n","- 최적의 물리적 실행계획을 선정한 다음, 저수준 프로그래밍 인터페이스인 RDD를 대상으로 모든 코드를 실행\n","    - 이 때 런타임 전체 태스크나 스테이지를 제거할 수 있는 자바 바이트 코드를 생성하여 추가적인 최적화를 수행하고 결과를 반환함"]},{"cell_type":"code","metadata":{"id":"qxCY_MAdJ8X4"},"source":[""],"execution_count":null,"outputs":[]}]}